{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"About","date":"2022-05-22T06:06:58.956Z","updated":"2022-05-22T06:06:58.956Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-05-22T06:06:58.959Z","updated":"2022-05-22T06:06:58.959Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-05-22T06:06:58.966Z","updated":"2022-05-22T06:06:58.966Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kafka API使用方法","slug":"Kafka-API使用方法","date":"2022-06-08T03:38:35.000Z","updated":"2022-06-08T03:46:18.959Z","comments":true,"path":"2022/06/08/Kafka-API使用方法/","link":"","permalink":"http://example.com/2022/06/08/Kafka-API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/","excerpt":"","text":"三.生产者API1、一个正常的生产逻辑需要具备以下几个步骤；（1）配置生产者客户端参数及创建相应的生产者实例（2）构建待发送的消息（3）发送消息（4）关闭生产者实例2、生产者API采用默认分区方式将消息散列的发送到各个分区当中；3、Properties props &#x3D; new Properties();&#x2F;&#x2F;配置生产者客户端参数；4、props.put(“bootstrap.servers”, “node1:9092,node2:9092, node3:9092”);&#x2F;&#x2F;设置kafka集群的地址5、acks 模式：取值0,1，-1（all）； 0：Producer往集群发送数据不需要等到集群的返回，不确保信息发送成功，安全性最低但是效率最高 1：Producer往集群发送数据只要Leader成功写入消息就能发送下一条，只确保Leader接收成功 -1（all）：确保Leader发送成功，所有的副本也发送成功，过程虽然缓慢但是安全性最高；6、props.put(“retries”, 3); &#x2F;&#x2F;失败重试次数，失败后会自动重试（可恢复&#x2F;不可恢复）→(有可能会造成数据的乱序)；7、props.put(“batch.size”, 10); &#x2F;&#x2F;数据发送的批次大小，提高效率&#x2F;吞吐量大会数据延迟；8、props.put(“linger.ms”, 10000); &#x2F;&#x2F;消息在缓冲区保留的时间,超过设置的值就会被提交到服务端；9、props.put(“max.request.size”,10); &#x2F;&#x2F;数据发送请求的最大缓存数；10、props.put(“buffer.memory”, 10240); &#x2F;&#x2F;整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端&#x2F;&#x2F;buffer.memory要大于batch.size,否则会报申请内存不足的错误降低阻塞的可能性；11、props.put(“key.serializer”,”org.apache.kafka.common. serialization.StringSerializer”);&#x2F;&#x2F;key-value序列化器；12、props.put(“value.serializer”, “org.apache.kafka.common. serialization.StringSerializer”);&#x2F;&#x2F;字符串最好；13、Kafka 生产者客户端 KatkaProducer 中的三个必要参数bootstrap.servers 、key.serializer 、value.serializer；14、生产者api参数发送方式（发后即忘）；发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。15、生产者api参数发送方式（同步发送）；producer.send(rcd).get( ); &#x2F;&#x2F;一旦调用get方法，就会阻塞Future future &#x3D; Callable.run( )&#x2F;&#x2F;有返回值，future.get（）runnable.run（）&#x2F;&#x2F;无返回值16、生产者api参数发送方式（异步发送）；回调函数会在producer收到 ack 时调用,为异步调用,该方法有两个参数,分别是RecordMetadata和Exception,如果Exception为null,说明消息发送成功,如果 Exception不为null,说明消息发送失败。注意：消息发送失败会自动重试,不需要我们在回调函数中手动重试。17、在IDEA中创建名为rgzn_kafka_zzl_0330的项目，在出现的pom.xml文件内添加需要的配置，重启rgzn_kafka_zzl_0330项目； 18、ProducerCallbackDemo类； 19、生产者原理； （1）一个生产者客户端由两个线程协调运行,这两个线程分别为主线程和 Sender 线程；（2）在主线程中由kafkaProducer创建消息,然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器(RecordAccumulator, 也称为消息收集器)中；（3）Sender线程负责从RecordAccumulator 获取消息并将其发送到Kafka中；（4）RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送, 进而减少网络传输的资源消耗以提升性能；（5）RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配置, 默认值为 33554432B ,即 32M；（6）主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列( Deque )中, RecordAccumulator 内部为每个分区都维护了一个双端队列,即 Deque。消息写入缓存时,追加到双端队列的尾部；（7）Sender 读取消息时,从双端队列的头部读取；（8）ProducerBatch是指一个消息批次; 与此同时,会将较小的 ProducerBatch凑成一个较大ProducerBatch ,也可以减少网络请求的次数以提升整体的吞吐量；（9）ProducerBatch大小和batch.size参数也有着密切的关系；（10）当一条消息(ProducerRecord ) 流入RecordAccumulator 时,会先寻找与消息分区所对应的双端队列(如果没有则新建),再从这个双端队列的尾部获取一个ProducerBatch (如果没有则新建),查看ProducerBatch 中是否还可以写入这个ProducerRecord,如果可以写入,如果不可以则需要创建一个新的 Producer Batch；（11）在新建ProducerBatch 时评估这条消息的大小是否超过batch.size 参数大小, 如果不超过, 那么就以batch.size参数的大小来创建ProducerBatch；（12）Sender 从 RecordAccumulator 获取缓存的消息之后,会进一步将&lt;分区,Deque&gt;的形式转变成&lt;Node,List&lt; ProducerBatch&gt;的形式,其中 Node 表示 Kafka 集群 broker 节点；（13）对于网络连接来说,生产者客户端是与具体 broker 节点建立的连接,也就是向具体的 broker 节点发送消息,而并不关心消息属于哪一个分区；（14）而对于 KafkaProducer 的应用逻辑而言,我们只关注向哪个分区中发送哪些消息,所以在这里需要做一个应用逻辑层面到网络 I&#x2F;O 层面的转换；（15）在转换成&lt;Node, List&gt;的形式之后, Sender 会进一步封装成&lt;Node,Request&gt; 的形式, 这样就可以将 Request 请求发往各个 Node 了,这里的 Request 是 Kafka 各种协议请求；（16）请求在从sender 线程发往 Kafka 之前还会保存到 InFlightRequests中,InFlightRequests 保存对象的具体形式为 Map&lt;Nodeld, Deque&gt;,它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。20、重要的生产者参数；（1）max.request.size：这个参数用来限制生产者客户端能发送的消息的最大值,默认值为 1048576B ,即 lMB 一般情况下,这个默认值就可以满足大多数的应用场景了。（2）compression.type：用来指定消息的压缩方式,默认值为“none “,即默认情况下,消息不会被压缩还可以配置为 “gzip”,”snappy” 和 “lz4”，服务端也有压缩参数，先解压，再压缩对消息进行压缩可以极大地减少网络传输、降低网络 I&#x2F;O,从而提高整体的性能。（3）retries 和 retry.backoff.ms：retries 参数用来配置生产者重试的次数,默认值为 0,即在发生异常的时候不进行任何重试动作。重试还和另一个参数 retry.backoff.ms 有关,这个参数的默认值为 100,它用来设定两次重试之间的时间间隔,避免无效的频繁重试。（4）batch.size：每个 Batch 要存放 batch.size 大小的数据后,才可以发送出去。比如说 batch.size 默认值是 16KB,那么里面凑够 16KB 的数据才会发送。（5）linger.ms：用来指定生产者发送 ProducerBatch 之前等待更多消息( ProducerRecord )加入ProducerBatch 时间,默认值为 0。（6）enable.idempotence：是否开启幂等性功能,详见后续原理加强。（7）partitioner.classe：用来指定分区器,默认:org.apache.kafka. internals.DefaultPartitioner –》用hashcode分。 四.消费者API1、一个正常的消费逻辑需要具备以下几个步骤：（1）、配置消费者客户端参数（2）、创建相应的消费者实例（3）、订阅主题（4）、拉取消息并消费（5）、提交消费位移offest（6）、关闭消费者实例2、subscribe重载方法：（1）、前面两种通过集合的方式订阅一到多个topicPublic 、void、subscribe（collection、topics、ConsumerRebalanceListenerlistener）Public、void、subscribe(collectiontopics)（2）、后两种主要是采用正则的方式订阅一到多个topics（3） public voidSubscribe（Pattern pattern，ConsumerRebalancelistener listener）Publicviod void subscribe(Patternopattern)（3）、正则方式订阅主题(只要是tpc数字的形式，三位数字以内如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅。在之后的过程中，如果有人又创建了新的主题，并且主题名字与正表达式相匹配那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并可以处理不同的类型，那么这种订阅方式就很有效。利用正则表达式订阅主题，可实现动态订阅；3、assign订阅主题消费者不仅可以通过KafkaConsumersubscribe()方法订阅主题，还可直接订阅某些主题的指定分区:在KafkaConsumer中提供了assign方法来实现这些功能，此方法的具体定义如下:public void assign(Collectionpartitions);这个方法只接受参数partitions用来指定需要订阅的分区集合示例如下:consumer.assign(Arrays.asList(new TopicPartition (“tpc_1”,0),newTopicPartition(“tpc 2,1)));4、subscribe与assign的区别(1)通过subscribe0)方法订阅主题具有消费者自动再均衡功能:在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组的消费者增加或减少时，分区分配关系会自动调整以实现消费负载均衡及故障自动转移。(2)assign0)方法订阅分区时，是不具备消费者自动均衡的功能的;其实这一点从assign0方法参数可以看出端倪，两种类型subscribe都有 ConsumerRebalanceListener类型参数的方法。而assign()方法却没有。5、取消订阅(1)、可以使用KafkaConsumer中的unsubscribe)方法采取消主题的订阅.这个方法既可以取消通过subscribe(Collection)方式实现的订阅;(2)、也可以取消通过subscribe(Pattem)方式实现的订阅，还可以取消通过assign(Collection)方式实现的订阅。(3)、如果将subscribe(Collection)或assign(Collection)集合参数设置为空集合，作用与unsubscribe0)方法相同。如下示例中三行代码的效果相同: consumer.unsubscribe0;consumer.subscribe(new ArrayListO)consumer.assign(new ArrayListO)；6消息的消费增术Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法, poll() 方法返回的是所订阅的主题(分区)上的一组消息。对于 poll () 方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下:public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;topic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。offsset 表示消息在所属分区的偏移量。timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。headers 表示消息的头部内容。key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ;serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1;checksum 是 CRC32 的校验值。7、消息的消费模式示例代码片段 8、指定位移消费有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。seek()方法的具体定义如下:public void seek(TopicPartiton partition,long offset)9、在均衡监听器一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;10、自动位移提交Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 enable.auto.commit 配置, 默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置, 默认值为 5 秒, 此参数生效的前提是 enable.auto.commit 参数为 true。在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。（1）重复消费假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。（2）消息丢失按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。11、手动位移提交（调用kafak api）自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals ,示例如下props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, false);手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。（1）、同步提交对于采用 commitSync()的无参方法,它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的, 如果想寻求更细粒度的、 更精准的提交, 那么就需要使用 commitSync()的另一个有参方法, 具体定义如下:public void commitSync(final Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets)（2）异步提交方式commitSync()方法相反,异步提交的方式( commitAsync())在执行的时候消费者线程不会被阻塞;可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操 。异步提交以便消费者的性能得到一定的增强。 commitAsync 方法有一个不同的重载方法。12、其他重要参数fetch.min.bytes&#x3D;1B 一次拉取的最小字节数fetch.max.bytes&#x3D;50M 一次拉取的最大数据量fetch.max.wait.ms&#x3D;500ms 拉取时的最大等待时长max.partition.fetch.bytes &#x3D; 1MB 每个分区一次拉取的最大数据量max.poll.records&#x3D;500 一次拉取的最大条数connections.max.idle.ms&#x3D;540000ms 网络连接的最大闲置时长request.timeout.ms&#x3D;30000ms 一次请求等待响应的最大超时时间consumer 等待请求响应的最长时间metadata.max.age.ms&#x3D;300000 元数据在限定时间内没有进行更新,则会被强制更新reconnect.backoff.ms&#x3D;50ms 尝试重新连接指定主机之前的退避时间retry.backoff.ms&#x3D;100ms 尝试重新拉取数据的重试间隔isolation.level&#x3D;read_uncommitted 隔离级别! 决定消费者能读到什么样的数据read_uncommitted: 可以消费到 LSO(LastStableOffset)位置;read_committed: 可以消费到 HW(High Watermark)位置max.poll.interval.ms 超过时限没有发起 poll 操作,则消费组认为该消费者已离开消费组enable.auto.commit&#x3D;true 开启消费位移的自动提交auto.commit.interval.ms&#x3D;5000 自动提交消费位移的时间间隔 参考资料","categories":[],"tags":[]},{"title":"Kafka命令行操作","slug":"Kafka命令行操作","date":"2022-06-08T03:38:07.000Z","updated":"2022-06-08T03:45:26.598Z","comments":true,"path":"2022/06/08/Kafka命令行操作/","link":"","permalink":"http://example.com/2022/06/08/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/","excerpt":"","text":"二．Kafka命令行操作 1.创建topickafka-configs.sh –create –topic tpc_1 –partitions 2 –replication-factor2 –zookeeper node1:2181 2.删除topickafka-topics.sh –delete –topic tpc_1 –zookeeper node1:2181 3.查看topic(1)列出当前系统中的所有topickafka-topics.sh –zookeeper node1:2181-list(2)查看topic详细信息kafka-configs.sh –create –topic tpc_1 –zookeeper node1:2181–replication-factor0:14.增加分区数kafka-topics.sh –alter –topic tpc_1 –partitions 3 –zookeeper node1:21815.动态配置topic参数通过管理命令，可以为已创建的topic增加，修改，删除 topic level 参数（1）添加，修改配置参数kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –add-config compression.type&#x3D;gzip（2）删除配置参数kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –delete-config compression.type 参考资料","categories":[],"tags":[]},{"title":"Kafka环境配置","slug":"Kafka环境配置","date":"2022-06-08T03:37:32.000Z","updated":"2022-06-08T03:44:23.432Z","comments":true,"path":"2022/06/08/Kafka环境配置/","link":"","permalink":"http://example.com/2022/06/08/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"","text":"一．Kafka环境配置1.上传文件包 到&#x2F;export&#x2F;server&#x2F;2.解压文件：tar -zxvf kafka_2.11-2.0.0.tgz 3.创建软连接：ln -s kafka_2.11-2.0.0&#x2F; kafka 4.进入 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config 修改 配置文件 server.propertiescd &#x2F;export&#x2F;server&#x2F;kafka&#x2F;configvim server.properties 5.给 node2和 node3 scp 分发 kafkacd &#x2F;export&#x2F;server&#x2F;scp -r &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; node2:$PWDscp -r &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; node3:$PWD6.创建软连接ln -s &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; kafka7.配置 kafka 环境变量（注：可以一台一台配，也可以在 node1 完成后发给 node2和node3）vim &#x2F;etc&#x2F;profileexport KAFKA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;kafkaexport PATH&#x3D;$PATH:$KAFKA_HOME&#x2F;bin 8.重新加载环境变量source &#x2F;etc&#x2F;profile9.启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可)kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties 参考资料","categories":[],"tags":[]},{"title":"Spark流程配置","slug":"Spark流程配置","date":"2022-05-28T07:57:22.000Z","updated":"2022-05-28T08:18:09.653Z","comments":true,"path":"2022/05/28/Spark流程配置/","link":"","permalink":"http://example.com/2022/05/28/Spark%E6%B5%81%E7%A8%8B%E9%85%8D%E7%BD%AE/","excerpt":"","text":"五、Spark(local)模式Spark（local）本地模式是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时的环境Spark由四类角色组成整个Spark的运行环境●Master角色，管理整个集群的资源●Worker角色，管理单个服务器的资源●Driver角色，管理单个Spark任务在运行的时候的工作●Executor角色，单个任务运行的时候的工作者 Anaconda On Linux 安装（1）上传Anaconda3-2021.05-Linux-x86_64.sh文件到虚拟机&#x2F;export&#x2F;server&#x2F;目录下；使用cd命令进入到&#x2F;export&#x2F;server&#x2F;下，随后sh执行Anaconda3-2021.05-Linux-x86_64.sh 在遇到Do you accept the license terms? [yes | no]时，选择yes （2）在上述命令回车后，会让你选择你想要安装的路径，统一安装在&#x2F;export&#x2F;server&#x2F;anaconda3下； （3）等待执行完毕后，在新的[yes|no]选择界面选择yes；随后exit退出重新登录即可看到base，代表着安装完成； （4）创建虚拟环境pyspark，基于Python 3.8； （5）切换到虚拟环境内； （6）在虚拟环境内安装我们所需要的基本的一些包； 以上就是Anaconda的安装；Spark安装（1）上传Spark安装包到&#x2F;export&#x2F;server&#x2F;目录下；（2）使用cd命令进入到&#x2F;export&#x2F;server&#x2F;下，随后解压Spark包； （3）进入到&#x2F;etc&#x2F;profile&#x2F;中配置环境变量，添加以下内容 （4）编辑.bashrc文件，添加java和pyspark的Home值； （5）重新加载我们所添加的环境变量； （6）进入到&#x2F;export&#x2F;server&#x2F;pyspark&#x2F;bin目录下，运行.&#x2F;pyspark； 测试运行基于python的spark解释器环境 （7）通过查看4040端口得知它的运行状况。 六、Spark(stand-alone)模式stand-alone集群模式中，Spark的各个角色以独立进程的形式存在，并组成Spark集群环境，运行在Linux系统上StandAlone集群在进程上主要有三类●主节点Master进程：Master角色，管理整个集群资源，并托管各个任务的Driver●从节点Workers：Worker角色，管理每个机器的资源，分配对应资源来运行Executor（Task）●历史服务器HistoryServer：在Spark Application运行完成以后，保存事件日志数据至HDFS三台虚拟机安装Anaconda参考Spark（local）模式下的Anaconda的安装文档，在node2、node3完成对Anaconda的安装Spark（stand-alone）新配置（1）在node1节点上进入到&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf目录下；（2）将workers.template文件改名为workers； （3）修改workers内容，将localhost删除，在文本末添加以下内容； （4）将spark-env.sh.template文件改名为spark-env.sh； （5）在spar-env.sh文件末添加以下内容； （6）开启hadoop服务； （7）在HDFS上创建程序运行历史记录存放文件夹，已存在会显示File exists； （8）重新进入到&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;目录下，将spark-defaults.conf.template 文件改为spark-defaults.conf； （9）编辑spark-defaults.conf文件，修改添加以下内容； （10）将log4j.properties.template文件改为log4j.properties； （11）更改log4j.properties文件中的内容，将原本的INFO改为WARN； （12）将在node1上进行的操作分发各node2、node3； （13）在node2、node3上分别对分发的spark-3.1.2-bin-hadoop3.2进行软链接，软链接为spark；（14）启动历史服务器； （15）进入到18080端口，访问WebUI界面，查看所运行过的历史服务； （16）启动master和worker; （17）通过jps查看进程是否开启master和worker进程 （18）进入到8080端口，访问WebUI界面 七、Spark(HA)模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题；Master故障后，集群就不可用。在HA模式下当Active的Master出现故障时,另外的一个Standby Master会被选举出来。 更改Spark配置（1）在虚拟机node1上进入到&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;目录下； （2）进入到spark-env.sh中，删除SPARK_MASTER_HOST&#x3D;node1，随后在文本末添加如下内容； （3）分发更改的文件到虚拟机node2、node3上 （4）在node1上启动zookeeper，（注：这里所需要的zookeeper版本是3.7，非此版本的zookeeper需要更新zookeeper） （5）启动master和worker进程 （6）通过jps查看3台主机所需要的进程是否开启； （7）访问WebUI界面，查看node1和node2的状态，可以看到node1的状态为ALIVE，node2状态为STANDBY （8）删除掉node1上master的16551进程号，jps查看端口是否删除； （9）刷新node2WebUI界面，发现其状态由STANDBY变为ALIVE； 八、Spark(Yarn)模式在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端Spark在YARN上的角色●Master角色由YARN的ResourceManager担任●Worker角色由YARN的NodeManager担任.●Driver角色运行在YARN容器内或提交任务的客户端进程中●Executor运行在YARN提供的容器内配置Spark（1）查看&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;下的spark-env.sh文件，查看是否配置变量，若未配置则在文末添加内容； （2）在YARN上运行spark； （3）上述表明可以在YARN集群上运行spark；（4）client 模式测试； （5）cluster 模式测试 （6）为了查看他的运行情况，需要开启hadoop的历史服务； （7）访问WebUI界面，查看client模式下的运行状况 （8）访问WebUI界面，查看cluster模式下的运行状况 参考资料","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-05-15T04:14:53.961Z","updated":"2022-05-15T04:14:53.961Z","comments":true,"path":"2022/05/15/hello-world/","link":"","permalink":"http://example.com/2022/05/15/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}