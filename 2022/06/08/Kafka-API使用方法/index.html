<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>Kafka API使用方法 | Hexo</title>
    
    
        <meta name="keywords" content="Kafka API使用方法" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="三.生产者API1、一个正常的生产逻辑需要具备以下几个步骤；（1）配置生产者客户端参数及创建相应的生产者实例（2）构建待发送的消息（3）发送消息（4）关闭生产者实例2、生产者API采用默认分区方式将消息散列的发送到各个分区当中；3、Properties props &#x3D; new Properties();&#x2F;&#x2F;配置生产者客户端参数；4、props.put(“bootstr">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka API使用方法">
<meta property="og:url" content="http://example.com/2022/06/08/Kafka-API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="三.生产者API1、一个正常的生产逻辑需要具备以下几个步骤；（1）配置生产者客户端参数及创建相应的生产者实例（2）构建待发送的消息（3）发送消息（4）关闭生产者实例2、生产者API采用默认分区方式将消息散列的发送到各个分区当中；3、Properties props &#x3D; new Properties();&#x2F;&#x2F;配置生产者客户端参数；4、props.put(“bootstr">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-06-08T03:38:35.000Z">
<meta property="article:modified_time" content="2022-06-08T03:46:18.959Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
    

    
        <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml" />
    

    
        <link rel="icon" href="/favicon.ico" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Hexo</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">首页</a>
                
                    <a class="main-nav-link" href="/archives">归档</a>
                
                    <a class="main-nav-link" href="/categories">分类</a>
                
                    <a class="main-nav-link" href="/tags">标签</a>
                
                    <a class="main-nav-link" href="/about">关于</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">首页</a></td>
                
                    <td><a class="main-nav-link" href="/archives">归档</a></td>
                
                    <td><a class="main-nav-link" href="/categories">分类</a></td>
                
                    <td><a class="main-nav-link" href="/tags">标签</a></td>
                
                    <td><a class="main-nav-link" href="/about">关于</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-Kafka-API使用方法" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
                        
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2022/06/08/Kafka-API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/">
            <time datetime="2022-06-08T03:38:35.000Z" itemprop="datePublished">2022-06-08</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/zthxxx/Wiki-site/raw/writing/source/_posts/Kafka-API使用方法.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/zthxxx/Wiki-site/edit/writing/source/_posts/Kafka-API使用方法.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/zthxxx/Wiki-site/commits/writing/source/_posts/Kafka-API使用方法.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Kafka API使用方法
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">Catalogue</strong>
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.</span> <span class="toc-text">参考资料</span></a></li></ol>
                </div>
            
        
        
            <p>三.生产者API<br>1、一个正常的生产逻辑需要具备以下几个步骤；<br>（1）配置生产者客户端参数及创建相应的生产者实例<br>（2）构建待发送的消息<br>（3）发送消息<br>（4）关闭生产者实例<br>2、生产者API采用默认分区方式将消息散列的发送到各个分区当中；<br>3、Properties props &#x3D; new Properties();&#x2F;&#x2F;配置生产者客户端参数；<br>4、props.put(“bootstrap.servers”, “node1:9092,node2:9092, node3:9092”);&#x2F;&#x2F;设置kafka集群的地址<br>5、acks 模式：取值0,1，-1（all）；<br>    0：Producer往集群发送数据不需要等到集群的返回，不确保信息发送成功，安全性最低但是效率最高<br>    1：Producer往集群发送数据只要Leader成功写入消息就能发送下一条，只确保Leader接收成功<br>    -1（all）：确保Leader发送成功，所有的副本也发送成功，过程虽然缓慢但是安全性最高；<br>6、props.put(“retries”, 3); &#x2F;&#x2F;失败重试次数，失败后会自动重试（可恢复&#x2F;不可恢复）→(有可能会造成数据的乱序)；<br>7、props.put(“batch.size”, 10); &#x2F;&#x2F;数据发送的批次大小，提高效率&#x2F;吞吐量大会数据延迟；<br>8、props.put(“linger.ms”, 10000); &#x2F;&#x2F;消息在缓冲区保留的时间,超过设置的值就会被提交到服务端；<br>9、props.put(“max.request.size”,10); &#x2F;&#x2F;数据发送请求的最大缓存数；<br>10、props.put(“buffer.memory”, 10240); &#x2F;&#x2F;整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端&#x2F;&#x2F;buffer.memory要大于batch.size,否则会报申请内存不足的错误降低阻塞的可能性；<br>11、props.put(“key.serializer”,”org.apache.kafka.common. serialization.StringSerializer”);&#x2F;&#x2F;key-value序列化器；<br>12、props.put(“value.serializer”, “org.apache.kafka.common. serialization.StringSerializer”);&#x2F;&#x2F;字符串最好；<br>13、Kafka 生产者客户端 KatkaProducer 中的三个必要参数bootstrap.servers 、key.serializer 、value.serializer；<br>14、生产者api参数发送方式（发后即忘）；<br>发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。<br>15、生产者api参数发送方式（同步发送）；<br>producer.send(rcd).get( ); &#x2F;&#x2F;一旦调用get方法，就会阻塞<br>Future  future &#x3D; Callable.run( )&#x2F;&#x2F;有返回值，future.get（）<br>runnable.run（）&#x2F;&#x2F;无返回值<br>16、生产者api参数发送方式（异步发送）；<br>回调函数会在producer收到 ack 时调用,为异步调用,该方法有两个参数,分别是RecordMetadata和Exception,如果Exception为null,说明消息发送成功,如果 Exception不为null,说明消息发送失败。<br>注意：消息发送失败会自动重试,不需要我们在回调函数中手动重试。<br>17、在IDEA中创建名为rgzn_kafka_zzl_0330的项目，在出现的pom.xml文件内添加需要的配置，重启rgzn_kafka_zzl_0330项目；</p>
<p>18、ProducerCallbackDemo类；</p>
<p>19、生产者原理；</p>
<p>（1）一个生产者客户端由两个线程协调运行,这两个线程分别为主线程和 Sender 线程；<br>（2）在主线程中由kafkaProducer创建消息,然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器(RecordAccumulator, 也称为消息收集器)中；<br>（3）Sender线程负责从RecordAccumulator 获取消息并将其发送到Kafka中；<br>（4）RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送, 进而减少网络传输的资源消耗以提升性能；<br>（5）RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配置, 默认值为 33554432B ,即 32M；<br>（6）主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列( Deque )中, RecordAccumulator 内部为每个分区都维护了一个双端队列,即 Deque<ProducerBatch>。消息写入缓存时,追加到双端队列的尾部；<br>（7）Sender 读取消息时,从双端队列的头部读取；<br>（8）ProducerBatch是指一个消息批次; 与此同时,会将较小的 ProducerBatch凑成一个较大ProducerBatch ,也可以减少网络请求的次数以提升整体的吞吐量；<br>（9）ProducerBatch大小和batch.size参数也有着密切的关系；<br>（10）当一条消息(ProducerRecord ) 流入RecordAccumulator 时,会先寻找与消息分区所对应的双端队列(如果没有则新建),再从这个双端队列的尾部获取一个ProducerBatch (如果没有则新建),查看ProducerBatch 中是否还可以写入这个ProducerRecord,如果可以写入,如果不可以则需要创建一个新的 Producer Batch；<br>（11）在新建ProducerBatch 时评估这条消息的大小是否超过batch.size 参数大小, 如果不超过, 那么就以batch.size参数的大小来创建ProducerBatch；<br>（12）Sender 从 RecordAccumulator 获取缓存的消息之后,会进一步将&lt;分区,Deque<Producer Batch>&gt;的形式转变成&lt;Node,List&lt; ProducerBatch&gt;的形式,其中 Node 表示 Kafka 集群 broker 节点；<br>（13）对于网络连接来说,生产者客户端是与具体 broker 节点建立的连接,也就是向具体的 broker 节点发送消息,而并不关心消息属于哪一个分区；<br>（14）而对于 KafkaProducer 的应用逻辑而言,我们只关注向哪个分区中发送哪些消息,所以在这里需要做一个应用逻辑层面到网络 I&#x2F;O 层面的转换；<br>（15）在转换成&lt;Node, List<ProducerBatch>&gt;的形式之后, Sender 会进一步封装成&lt;Node,Request&gt; 的形式, 这样就可以将 Request 请求发往各个 Node 了,这里的 Request 是 Kafka 各种协议请求；<br>（16）请求在从sender 线程发往 Kafka 之前还会保存到 InFlightRequests中,InFlightRequests 保存对象的具体形式为 Map&lt;Nodeld, Deque<request>&gt;,它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。<br>20、重要的生产者参数；<br>（1）max.request.size：这个参数用来限制生产者客户端能发送的消息的最大值,默认值为 1048576B ,即 lMB 一般情况下,这个默认值就可以满足大多数的应用场景了。<br>（2）compression.type：用来指定消息的压缩方式,默认值为“none “,即默认情况下,消息不会被压缩还可以配置为 “gzip”,”snappy” 和 “lz4”，服务端也有压缩参数，先解压，再压缩对消息进行压缩可以极大地减少网络传输、降低网络 I&#x2F;O,从而提高整体的性能。<br>（3）retries 和 retry.backoff.ms：retries 参数用来配置生产者重试的次数,默认值为 0,即在发生异常的时候不进行任何重试动作。重试还和另一个参数 retry.backoff.ms 有关,这个参数的默认值为 100,它用来设定两次重试之间的时间间隔,避免无效的频繁重试。<br>（4）batch.size：每个 Batch 要存放 batch.size 大小的数据后,才可以发送出去。比如说 batch.size 默认值是 16KB,那么里面凑够 16KB 的数据才会发送。<br>（5）linger.ms：用来指定生产者发送 ProducerBatch 之前等待更多消息( ProducerRecord )加入ProducerBatch 时间,默认值为 0。<br>（6）enable.idempotence：是否开启幂等性功能,详见后续原理加强。<br>（7）partitioner.classe：用来指定分区器,默认:org.apache.kafka. internals.DefaultPartitioner –》用hashcode分。</p>
<p>四.消费者API<br>1、一个正常的消费逻辑需要具备以下几个步骤：<br>（1）、配置消费者客户端参数<br>（2）、创建相应的消费者实例<br>（3）、订阅主题<br>（4）、拉取消息并消费<br>（5）、提交消费位移offest<br>（6）、关闭消费者实例<br>2、subscribe重载方法：<br>（1）、前面两种通过集合的方式订阅一到多个topic<br>Public 、void、subscribe（collection<sting>、topics、ConsumerRebalanceListenerlistener）<br>Public、void、subscribe(collection<Sting>topics)<br>（2）、后两种主要是采用正则的方式订阅一到多个topics<br>（3） public voidSubscribe（Pattern pattern，ConsumerRebalancelistener listener）Publicviod void subscribe(Patternopattern)<br>（3）、正则方式订阅主题(只要是tpc数字的形式，三位数字以内<br>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅。在之后的过程中，如果有人又创建了新的主题，并且主题名字与正表达式相匹配那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并可以处理不同的类型，那么这种订阅方式就很有效。利用正则表达式订阅主题，可实现动态订阅；<br>3、assign订阅主题<br>消费者不仅可以通过KafkaConsumersubscribe()方法订阅主题，还可直接订阅某些主题的指定分区:在KafkaConsumer中提供了assign方法来实现这些功能，此方法的具体定义如下:public void assign(Collection<TopicPartition>partitions);<br>这个方法只接受参数partitions用来指定需要订阅的分区集合示例如下:<br>consumer.assign(Arrays.asList(new TopicPartition (“tpc_1”,0),new<br>TopicPartition(“tpc 2,1)));<br>4、subscribe与assign的区别<br>(1)通过subscribe0)方法订阅主题具有消费者自动再均衡功能:<br>在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组的消费者增加或减少时，分区分配关系会自动调整以实现消费负载均衡及故障自动转移。<br>(2)assign0)方法订阅分区时，是不具备消费者自动均衡的功能的;<br>其实这一点从assign0方法参数可以看出端倪，两种类型subscribe都有 ConsumerRebalanceListener类型参数的方法。而assign()方法却没有。<br>5、取消订阅<br>(1)、可以使用KafkaConsumer中的unsubscribe)方法采取消主题的订阅.这个方法既可以取消通过subscribe(Collection)方式实现的订阅;<br>(2)、也可以取消通过subscribe(Pattem)方式实现的订阅，还可以取消通过assign(Collection)方式实现的订阅。<br>(3)、如果将subscribe(Collection)或assign(Collection)集合参数设置为空集合，作用与unsubscribe0)方法相同。如下示例中三行代码的效果相同: consumer.unsubscribe0;<br>consumer.subscribe(new ArrayList<String>O)<br>consumer.assign(new ArrayList<TopicPartition>O)；<br>6消息的消费增术<br>Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。<br>推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。<br>Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法, poll() 方法返回的是所订阅的主题(分区)上的一组消息。对于 poll () 方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下:<br>public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)<br>超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;<br>topic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。<br>offsset 表示消息在所属分区的偏移量。<br>timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。<br>timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。<br>headers 表示消息的头部内容。<br>key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ;<br>serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1;<br>checksum 是 CRC32 的校验值。<br>7、消息的消费模式<br>示例代码片段</p>
<p>8、指定位移消费<br>有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。<br>seek()方法的具体定义如下:<br>public void seek(TopicPartiton partition,long offset)<br>9、在均衡监听器<br>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;<br>10、自动位移提交<br>Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 enable.auto.commit 配置, 默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置, 默认值为 5 秒, 此参数生效的前提是 enable.auto.commit 参数为 true。<br>在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。<br>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。<br>（1）重复消费<br>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。<br>（2）消息丢失<br>按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。<br>11、手动位移提交（调用kafak api）<br>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。<br>很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals ,示例如下<br>props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, false);<br>手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。<br>（1）、同步提交<br>对于采用 commitSync()的无参方法,它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的, 如果想寻求更细粒度的、 更精准的提交, 那么就需要使用 commitSync()的另一个有参方法, 具体定义如下:<br>public void commitSync(final Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets)<br>（2）异步提交方式<br>commitSync()方法相反,异步提交的方式( commitAsync())在执行的时候消费者线程不会被阻塞;可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操 。异步提交以便消费者的性能得到一定的增强。 commitAsync 方法有一个不同的重载方法。<br>12、其他重要参数<br>fetch.min.bytes&#x3D;1B		 		一次拉取的最小字节数<br>fetch.max.bytes&#x3D;50M 				一次拉取的最大数据量<br>fetch.max.wait.ms&#x3D;500ms 			拉取时的最大等待时长<br>max.partition.fetch.bytes &#x3D; 1MB 		每个分区一次拉取的最大数据量<br>max.poll.records&#x3D;500				一次拉取的最大条数<br>connections.max.idle.ms&#x3D;540000ms 	网络连接的最大闲置时长<br>request.timeout.ms&#x3D;30000ms 一次请求等待响应的最大超时时间consumer 等待请求响应的最长时间<br>metadata.max.age.ms&#x3D;300000 	元数据在限定时间内没有进行更新,则会被强制更新<br>reconnect.backoff.ms&#x3D;50ms 		尝试重新连接指定主机之前的退避时间<br>retry.backoff.ms&#x3D;100ms 		尝试重新拉取数据的重试间隔<br>isolation.level&#x3D;read_uncommitted 		隔离级别! 决定消费者能读到什么样的数据<br>read_uncommitted:				可以消费到 LSO(LastStableOffset)位置;<br>read_committed:				可以消费到 HW(High Watermark)位置<br>max.poll.interval.ms 			超过时限没有发起 poll 操作,则消费组认为该消费者已离开消费组<br>enable.auto.commit&#x3D;true 			开启消费位移的自动提交<br>auto.commit.interval.ms&#x3D;5000 		自动提交消费位移的时间间隔</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote>
<ul>
<li><a href=""></a></li>
<li><a href=""></a></li>
</ul>
</blockquote>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/2022/06/21/Kafka-eagle%E9%85%8D%E7%BD%AE%E6%96%87%E6%A1%A3/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    Kafka-eagle配置文档
                
            </div>
        </a>
    
    
        <a href="/2022/06/08/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">Kafka命令行操作</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            John Doe &copy; 2022 
            <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a target="_blank" rel="noopener" href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
</body>
</html>